---
title: "Problem 1"
author: "Marcy Anderson"
date: "4/18/2022"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2) 
library(dplyr)
library(ggplot2)
library(tidyr)
library(leaps)
library(glmnet)
library(car)
```

QUESTIONS: 

How do we determine what size to use for subset selection or what lambda to use for ridge/lasso when using existing methods gives us over 80 columns (in part because of factors)?

What type of information should we include in the methodology section? Should we be talking about how the methodology works? how we're using it? Why we're using it? What our tuning param etc are? 




```{r, include = FALSE}
diamonds = read.csv("C:/Users/thing1/Documents/SCHOOL/4th_year/DS 301/DS301FinalProject/diamonds.csv",header=TRUE)

head(diamonds)

some_col = subset(diamonds,select=c(size, color, clarity, cut, 
                                    depth_percent, table_percent, meas_width, meas_length, meas_depth, total_sales_price))

model_of_diamonds = lm(total_sales_price~., data = some_col)
vif(model_of_diamonds)


diamonds$widthlength = diamonds$meas_width * diamonds$meas_length
some_col = subset(diamonds,select=c( color, clarity, cut, 
                                    depth_percent, table_percent, widthlength, meas_depth, total_sales_price))
model_of_diamonds = lm(total_sales_price~., data = some_col)
vif(model_of_diamonds)

ggplot(data = diamonds, aes(meas_width, meas_depth))+geom_point(color = 'blue', alpha = 0.1)+facet_wrap(~shape)
ggplot(data = diamonds, aes(meas_width, meas_length))+geom_point(color = 'blue', alpha = 0.1)#+facet_wrap(~shape)
ggplot(data = diamonds, aes(widthlength, size))+geom_point(color = 'blue', alpha = 0.1)


```

Best subset
```{r}
head(some_col)
## get only the diamonds under 10k
#priced_diamonds = some_col %>% filter(total_sales_price <10000)


#Best subset search 
bestset = regsubsets(total_sales_price~.,nvmax = 33, data=some_col, really.big=T)
bestset.sum = summary(bestset)

n = dim(some_col)[1]
p = rowSums(bestset.sum$which)

brss = bestset.sum$rss
bAIC = n*log(brss/n)+2*(p)
bBIC = n*log(brss/n)+(p)*log(n)
which.min(bAIC)
bAIC[28]
which.min(bBIC)
bBIC[24]
which.max(bestset.sum$adjr2)
bestset.sum$adjr2[28]
which.min(bestset.sum$cp)
bestset.sum$cp[28]
coef(bwdset,24)
```

forward selection
```{r}
#### BACKWARD SELECTION
bwdset = regsubsets(total_sales_price~.,data=some_col, nvmax=50, method = "backward")

bwdset.sum = summary(bwdset)

n = dim(some_col)[1]
p = rowSums(bwdset.sum$which)

brss = bwdset.sum$rss
bAIC = n*log(brss/n)+2*(p)
bBIC = n*log(brss/n)+(p)*log(n)
which.min(bAIC)
bAIC[28]
which.min(bBIC)
bBIC[24]
which.max(bwdset.sum$adjr2)
bwdset.sum$adjr2[28]
which.min(bwdset.sum$cp)
bwdset.sum$cp[28]
coef(bwdset,24)
```

backward selection
```{r}
#### FORWARD SELECTION

fwdset = regsubsets(total_sales_price~.,data=some_col, nvmax=50, method = "forward")

fwdset.sum = summary(fwdset)

n = dim(some_col)[1]
p = rowSums(fwdset.sum$which)

frss = fwdset.sum$rss
fAIC = n*log(frss/n)+2*(p)
fBIC = n*log(frss/n)+(p)*log(n)
which.min(fAIC)
fAIC[28]
which.min(fBIC)
fBIC[24]
which.max(fwdset.sum$adjr2)
fwdset.sum$adjr2[28]
which.min(fwdset.sum$cp)
fwdset.sum$cp[28]
coef(fwdset,24)

```


Ridge and lasso

```{r}
## RIDGE

train = sample(1:nrow(some_col), nrow(some_col)/2)
test=(-train)
train_data = some_col[train,]
test_data = some_col[-train,]

grid = 10^seq(10,-2,length=100)

x = model.matrix(total_sales_price ~.,data=some_col)[,-1]

Y = some_col$total_sales_price

ridge.train = glmnet(x[train,],Y[train],alpha=0,lambda=grid)
lasso.train = glmnet(x[train,],Y[train],alpha=1,lambda=grid)


cv.out.ridge = cv.glmnet(x[train,],Y[train],alpha = 0, lambda = grid) 
cv.out.lasso = cv.glmnet(x[train,],Y[train],alpha = 1, lambda = grid) 
bestlambdaridge = cv.out.ridge$lambda.min
bestlambdalasso = cv.out.lasso$lambda.min
selambdaridge = cv.out.ridge$lambda.1se
selambdalasso = cv.out.lasso$lambda.1se


ridge.pred1 = predict(cv.out.ridge,s=bestlambdaridge,newx=x[test,])
mean((ridge.pred1-Y[test])^2)
lasso.pred1 = predict(cv.out.lasso,s=bestlambdalasso,newx=x[test,])
mean((lasso.pred1-Y[test])^2)
ridge.pred2 = predict(cv.out.ridge,s=selambdaridge,newx=x[test,])
mean((ridge.pred2-Y[test])^2)
lasso.pred2 = predict(cv.out.lasso,s=selambdalasso,newx=x[test,])
mean((lasso.pred2-Y[test])^2)
bestlambdalasso
which(grid==bestlambdalasso)
cv.out.lasso$lambda[56]
coef(lasso.train)[,56]


```
best ridge mse: [1] 441663744
best lasso mse: [1] 439893686
se ridge mse: [1] 545955317
se lasso mse: [1] 542035445

of the four we tried the lowest Test MSE is the minimum value for the lambda test
